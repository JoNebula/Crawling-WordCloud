# -*- coding: utf-8 -*-
"""Crawling_and_WordCloud.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MtHHCln0luJrKZJQmPWX3j7qznPfiVbF
"""

from bs4 import BeautifulSoup as bs
import urllib.request

#import requests
import time
import re
import NaverCrawler



client_id = "HSGXhbVLnjvb31S9N_cB"  # 발급받은 client id
client_secret = "4r9gnASzKU"

URLS = NaverCrawler.GetBlogURL("아이방 인테리어",client_id,client_secret,size=20)
print(URLS)

article = []
for URL in URLS:
    cont,_ = NaverCrawler.GetNaverBloginfo(URL)
    article.append(cont)

"""# Word Cloud"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import re
import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
stopword_list = nltk.corpus.stopwords.words('english')
from nltk.stem.wordnet import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
from nltk.corpus import wordnet

from wordcloud import WordCloud
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from collections import Counter
import datetime

!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# apt-get update
# apt-get install g++ openjdk-8-jdk python-dev python3-dev
# pip3 install JPype1
# pip3 install konlpy

# Commented out IPython magic to ensure Python compatibility.
# %env JAVA_HOME "/usr/lib/jvm/java-8-openjdk-amd64"

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)
# pip3 install /tmp/mecab-python-0.996

!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

df = pd.DataFrame()
df["raw"] = article

cleaned_article = df["raw"].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")
df["cleaned"] = cleaned_article

df

total = " ".join(df["cleaned"])

from konlpy.tag import Mecab


stopwords=['의','가','이','고','은','을','라','하','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다','는데요','답니다','아주']

tokenizer = Mecab()
tokenized=[]

temp = tokenizer.morphs(total) # 토큰화
temp = [word for word in temp if (not word in stopwords) and (len(word)>=2)] # 불용어 제거
tokenized.append(temp)

cleaned = tokenized

# Plot word frequency distribution of first few words
plt.figure(figsize=(16,9))
plt.xticks(fontsize=10, rotation=90, family='NanumBarunGothic')
freq_dist = nltk.FreqDist(cleaned[0])
freq_dist.plot(50, cumulative=False)

# Word Cloud with word frequencies

wordcloud = WordCloud(width=900,
                      height=500,
                      max_words=500,
                      max_font_size=100,
                      relative_scaling=0.5,
                      colormap='Blues',
                      font_path = 'NanumGothic',
                      normalize_plurals=True).generate_from_frequencies(freq_dist)
plt.figure(figsize=(16,9))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()